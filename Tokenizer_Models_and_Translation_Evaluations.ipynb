{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg1rrTXYe0tsl7NhMAy3wS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebinbusra/Translation/blob/main/Tokenizer_Models_and_Translation_Evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Import necessary libraries and modules**"
      ],
      "metadata": {
        "id": "rkzOY_Zay9Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, decoders\n",
        "import unicodedata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXPnnhKSKPpw",
        "outputId": "40a4e56b-3212-4ab6-c9dc-3a04c282e4d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBOCI0DlRUTZ",
        "outputId": "223d58f3-9e99-4d39-ea01-bef417e16eb6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. TOKENIZER TYPES**"
      ],
      "metadata": {
        "id": "YgcqgMFRzGv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. BPE TOKENIZER"
      ],
      "metadata": {
        "id": "KfcH4Vvjw0TN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p4we9bzJ7fU",
        "outputId": "210de4c0-b567-4084-89a8-43fece380b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Applying normalization to clear invalid characters:\n",
            "['Merhaba! Nasılsın?', 'Bugün hava çok güzel.', 'Yarın buluşmak için plan yapıyoruz.', 'Türk mutfağı dünya çapında ünlüdür.', 'Köpeğimle parkta gezinti yapıyorum.', 'Bir fincan kahve içer misin?', 'Okula gitmek için erken kalkmalıyız.', 'Sonbahar renkleri harika.', 'Dizi izlemekten hoşlanıyorum.', 'Eski dostlarla buluşmak her zaman keyiflidir.']\n",
            "\n",
            "Step 2: Trained Tokenizer:\n",
            "{'Okula gitmek için erken kalkmalıyız.': 263, 'ula g': 183, 'kta g': 194, 'parkta gezinti yapıyorum.': 253, 'st': 158, 'kahv': 181, 'el': 113, 'mek için ': 196, 'ı dü': 167, 'ıyoruz.': 184, 'iç': 57, 'ezinti yapıyorum.': 251, 'l': 29, 'Bir': 96, 'buluşmak için plan yapıyoruz.': 240, 'y': 39, 'şlan': 169, 'k her ': 174, 'uş': 79, 'imle ': 218, 'can ': 109, 'i iz': 175, 'erken ': 173, 'n': 31, 'lemekten ho': 225, 'ız.': 166, 'Sonbahar renkl': 248, 'p': 33, 'ı dünya çapında ünlüdür.': 254, 'iz': 73, 'er ': 84, 'mutfağ': 226, 'yapıyorum.': 197, '! N': 94, 'Yar': 104, 'dost': 215, 'içer ': 180, 'misin?': 227, '[CLS]': 2, 'k ': 51, 'n?': 147, 'k için plan ': 200, 'ında ': 168, 'şlanıyorum.': 235, 'r': 34, 'Köpeğimle ': 245, 'Bugün ha': 204, 'Eski dostlarla buluşmak her zaman keyiflidir.': 266, 'Merhaba! Nasılsın?': 262, 'an ': 63, 'buluşmak her ': 201, 'lü': 135, 's': 35, 'apında ': 176, ' r': 92, 'ki ': 131, ' ': 5, 'kten ho': 195, 'dir': 112, 'nya çapında ': 228, 'r.': 156, 'do': 110, 'O': 15, 'Bir fincan kahve içer ': 242, 'ıyorum.': 86, 'Es': 98, 'So': 102, 'ap': 55, 'buluşma': 89, 'Merhaba! N': 209, 'm.': 76, 'Diz': 97, 'i': 27, 'ü': 43, 'ka': 58, 'i ': 52, 'ç': 41, 'D': 10, 'kl': 130, 'ey': 115, 'eri harika.': 255, 'eğ': 117, 'kalkma': 182, 'en': 114, 'le ': 140, 'n ha': 170, 'Türk ': 212, 'tfağ': 160, 'ken ': 133, 'lar': 137, 'e ': 67, 'ma': 53, 'gü': 69, 'harika': 193, 'ıyor': 61, 'eri harika': 236, '.': 7, '!': 6, 'an': 105, 'ünlüdür.': 233, 'ıyoru': 62, 'haba! N': 191, ' N': 90, 'ş': 46, 'en ho': 188, 'ok ': 152, 'ö': 42, 'Bugün hava çok güzel': 241, 'parkta g': 230, 'va ç': 161, 'ho': 121, 'Bir fincan kahve içer misin?': 258, 'Merhaba! Nasılsı': 246, 'en ': 68, 'plan ': 155, 'Eski dost': 244, 'va çok güzel': 231, '[MASK]': 4, 'Dizi iz': 206, 'her ': 123, 'm': 30, 'buluş': 87, 'asılsı': 214, 'gün ha': 190, 'ünlü': 164, 'eme': 118, '[PAD]': 0, 'Türk mutfağ': 249, 'Okula g': 210, 'ama': 107, 'mu': 145, 'a g': 82, 'k': 28, 'me': 77, 'eri ': 172, 'lan ': 139, 'Bir fincan ': 205, 'Dizi izlemekten hoşlanıyorum.': 259, 'erken kalkma': 237, 'T': 17, 'Kö': 99, 'iflidir.': 221, 'e içer ': 187, 'Bu': 95, 'Sonba': 211, 'nba': 149, 'ı dünya çapında ': 234, 'Yarın buluşmak için plan yapıyoruz.': 250, 'fağ': 120, 'mis': 146, 'nda ': 150, 'S': 16, 'it': 126, 'larla buluşmak her ': 223, 'er': 50, 'idir.': 220, 'har renkl': 239, 'ika': 127, 'd': 22, '[UNK]': 1, ' f': 91, 'ny': 148, 'par': 153, 'rk ': 157, 'buluşmak için plan ': 202, 'Tü': 103, 'ba': 64, 'ti ': 159, 'yapıyoruz.': 198, 'lsı': 141, 'or': 59, 'ul': 60, 'har': 71, 'Ok': 101, 'lıyız.': 224, 'ar': 49, 'itmek için ': 219, 'la ': 136, 'g': 25, ' dü': 93, 'güzel': 189, 'o': 32, 'Eski dostlarla buluşmak her zaman key': 260, 'c': 21, 'ası': 108, 'h': 26, 'erken kalkmalıyız.': 256, 'ir': 72, 'kt': 75, 'leme': 143, 'ezinti ': 217, 'Mer': 100, 'sı': 78, 'Okula gitmek için ': 247, 'v': 38, 'is': 125, 'kma': 132, 'a': 19, 'zel': 163, 'hv': 122, 'peğ': 154, 'ok güzel': 229, 'Türk mutfağı dünya çapında ünlüdür.': 265, 'n ': 47, 'dür.': 186, 'in?': 177, 'u': 37, 'in': 56, 'enkl': 216, 'E': 11, 'dü': 66, 'Köpeğ': 208, 'ba! N': 185, 'B': 9, 'Sonbahar renkleri harika.': 264, 'Bugün hava çok güzel.': 257, 't': 36, 'Köpeğimle parkta gezinti yapıyorum.': 261, 'zaman key': 232, 'f': 24, 'bul': 65, 'zama': 162, 'da ': 111, 'a ': 48, 'har r': 192, 'a ç': 83, 'ıy': 54, 'a çapında ': 199, 'b': 20, 'key': 134, 'n key': 171, 'z': 40, 'kahve içer ': 238, 'Dizi izlemekten ho': 243, 'ı': 45, 'ez': 116, 'inti ': 179, 'ağ': 106, 'ifl': 129, 'z.': 81, 'in ': 74, '[SEP]': 3, 'incan ': 178, 'nlü': 151, 'fl': 119, ' fincan ': 203, 'Eski ': 207, 'la buluşmak her ': 222, 'N': 14, 'Y': 18, 'ğ': 44, 'lıy': 138, 'ın ': 165, 'im': 124, 'larla buluşmak her zaman key': 252, 'k için ': 88, 'ha': 70, 'idir': 128, 'K': 12, 'k iç': 85, 'yap': 80, '?': 8, 'e': 23, 'M': 13, 'lan': 142, 'lkma': 144, 'Yarın ': 213}\n",
            "\n",
            "Step 3: Tokenized Pretraining Data:\n",
            "[[30, 50, 70, 64, 6, 5, 31, 214, 147], [20, 37, 190, 231, 7], [39, 49, 165, 240], [36, 43, 157, 226, 254], [28, 42, 154, 218, 253], [20, 72, 203, 238, 227], [32, 28, 183, 219, 256], [35, 32, 149, 239, 255], [22, 73, 175, 225, 235], [23, 35, 131, 215, 252, 221]]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Applying normalization to clear invalid characters from the text\n",
        "def apply_normalization(text):\n",
        "    # Normalize the text to NFC (Normalization Form C)\n",
        "    normalized_text = unicodedata.normalize(\"NFC\", text)\n",
        "    return normalized_text\n",
        "\n",
        "# Step 2: Training the tokenizer according to the predetermined vocabulary size\n",
        "def train_tokenizer(corpus, vocab_size):\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "    trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"], vocab_size=vocab_size)\n",
        "    tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# Step 3: Processing the corpus with the trained tokenizer to obtain a tokenized pretraining data\n",
        "def preprocess_with_tokenizer(tokenizer, text):\n",
        "    # Lowercase conversion\n",
        "    text = text.lower()\n",
        "\n",
        "    # NFC normalization\n",
        "    normalized_text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    encoded_text = tokenizer.encode(normalized_text)\n",
        "    return encoded_text.ids\n",
        "\n",
        "# Sample usage\n",
        "corpus = [\n",
        "    \"Merhaba! Nasılsın?\",\n",
        "    \"Bugün hava çok güzel.\",\n",
        "    \"Yarın buluşmak için plan yapıyoruz.\",\n",
        "    \"Türk mutfağı dünya çapında ünlüdür.\",\n",
        "    \"Köpeğimle parkta gezinti yapıyorum.\",\n",
        "    \"Bir fincan kahve içer misin?\",\n",
        "    \"Okula gitmek için erken kalkmalıyız.\",\n",
        "    \"Sonbahar renkleri harika.\",\n",
        "    \"Dizi izlemekten hoşlanıyorum.\",\n",
        "    \"Eski dostlarla buluşmak her zaman keyiflidir.\"\n",
        "]\n",
        "\n",
        "# Step 1: Applying normalization to clear invalid characters from the text\n",
        "corpus_normalized = [apply_normalization(text) for text in corpus]\n",
        "print(\"Step 1: Applying normalization to clear invalid characters:\")\n",
        "print(corpus_normalized)\n",
        "\n",
        "# Step 2: Training the tokenizer according to the predetermined vocabulary size\n",
        "vocab_size = 1000  # You can adjust the vocabulary size based on your requirements\n",
        "tokenizer = train_tokenizer(corpus_normalized, vocab_size)\n",
        "print(\"\\nStep 2: Trained Tokenizer:\")\n",
        "print(tokenizer.get_vocab())\n",
        "\n",
        "# Step 3: Processing the corpus with the trained tokenizer to obtain a tokenized pretraining data\n",
        "preprocessed_data = [preprocess_with_tokenizer(tokenizer, text) for text in corpus_normalized]\n",
        "print(\"\\nStep 3: Tokenized Pretraining Data:\")\n",
        "print(preprocessed_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. WORDPIECE TOKENIZER"
      ],
      "metadata": {
        "id": "nxoggvoHyzo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, decoders\n",
        "import unicodedata\n",
        "\n",
        "# Step 1: Applying normalization to clear invalid characters from the text\n",
        "def apply_normalization(text):\n",
        "    # Normalize the text to NFC (Normalization Form C)\n",
        "    normalized_text = unicodedata.normalize(\"NFC\", text)\n",
        "    return normalized_text\n",
        "\n",
        "# Step 2: Training the tokenizer according to the predetermined vocabulary size\n",
        "def train_tokenizer(corpus, vocab_size):\n",
        "    tokenizer = Tokenizer(models.WordPiece())\n",
        "    trainer = trainers.WordPieceTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"], vocab_size=vocab_size)\n",
        "    tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# Step 3: Processing the corpus with the trained tokenizer to obtain a tokenized pretraining data\n",
        "def preprocess_with_tokenizer(tokenizer, text):\n",
        "    # Lowercase conversion\n",
        "    text = text.lower()\n",
        "\n",
        "    # NFC normalization\n",
        "    normalized_text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    encoded_text = tokenizer.encode(normalized_text)\n",
        "    return encoded_text.ids\n",
        "\n",
        "# Sample usage\n",
        "corpus = [\n",
        "    \"Merhaba! Nasılsın?\",\n",
        "    \"Bugün hava çok güzel.\",\n",
        "    \"Yarın buluşmak için plan yapıyoruz.\",\n",
        "    \"Türk mutfağı dünya çapında ünlüdür.\",\n",
        "    \"Köpeğimle parkta gezinti yapıyorum.\",\n",
        "    \"Bir fincan kahve içer misin?\",\n",
        "    \"Okula gitmek için erken kalkmalıyız.\",\n",
        "    \"Sonbahar renkleri harika.\",\n",
        "    \"Dizi izlemekten hoşlanıyorum.\",\n",
        "    \"Eski dostlarla buluşmak her zaman keyiflidir.\"\n",
        "]\n",
        "\n",
        "# Step 1: Applying normalization to clear invalid characters from the text\n",
        "corpus_normalized = [apply_normalization(text) for text in corpus]\n",
        "print(\"Step 1: Applying normalization to clear invalid characters:\")\n",
        "print(corpus_normalized)\n",
        "\n",
        "# Step 2: Training the tokenizer according to the predetermined vocabulary size\n",
        "vocab_size = 1000  # You can adjust the vocabulary size based on your requirements\n",
        "tokenizer = train_tokenizer(corpus_normalized, vocab_size)\n",
        "print(\"\\nStep 2: Trained Tokenizer:\")\n",
        "print(tokenizer.get_vocab())\n",
        "\n",
        "# Step 3: Processing the corpus with the trained tokenizer to obtain a tokenized pretraining data\n",
        "preprocessed_data = [preprocess_with_tokenizer(tokenizer, text) for text in corpus_normalized]\n",
        "print(\"\\nStep 3: Tokenized Pretraining Data:\")\n",
        "print(preprocessed_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAeIWibUy40u",
        "outputId": "a0039850-6353-4286-9304-1a16f673934b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Applying normalization to clear invalid characters:\n",
            "['Merhaba! Nasılsın?', 'Bugün hava çok güzel.', 'Yarın buluşmak için plan yapıyoruz.', 'Türk mutfağı dünya çapında ünlüdür.', 'Köpeğimle parkta gezinti yapıyorum.', 'Bir fincan kahve içer misin?', 'Okula gitmek için erken kalkmalıyız.', 'Sonbahar renkleri harika.', 'Dizi izlemekten hoşlanıyorum.', 'Eski dostlarla buluşmak her zaman keyiflidir.']\n",
            "\n",
            "Step 2: Trained Tokenizer:\n",
            "{'##leme': 215, '##da ünlü': 259, 'Yarın ': 241, '## d': 147, '##ız.': 184, 'Kö': 128, 'r': 34, '##ti ': 170, 'p': 33, '##! N': 190, '##güze': 224, 'Dizi izlemekten ho': 274, 'K': 12, '##haba': 220, 'So': 131, '##lü': 152, '##kalk': 209, 'b': 20, 'Tür': 132, '## r': 143, '## ': 49, 'Es': 127, '##ho': 174, '##lk': 149, 'Mer': 129, '##?': 74, 'c': 21, '##v': 78, '##u': 63, '##iz': 96, '##k ': 83, 'Sonba': 239, 'Bir f': 233, 'Ok': 130, '##a g': 117, '##N': 72, '##l': 50, '##lar': 155, '##ler': 154, '##eğ': 160, '##i': 47, '##s': 73, 'f': 24, '##ha': 106, '##erken kalkmalıyız.': 286, '!': 6, '##en ho': 217, 'Bugün hava çok güze': 272, '##iflidir.': 282, 'Okula gitmek için ': 278, '##er mis': 229, '##ın?': 222, 'S': 16, '##ar': 84, '##malıyız.': 266, 'a': 19, '##peğ': 189, 'M': 13, '##k için ': 123, '##zaman keyiflidir.': 283, '##ö': 67, 'T': 17, '##ir.': 214, 'o': 32, '##d': 77, 'Bir': 125, 'Merhaba': 237, 'Eski dostlarla buluşmak her zaman keyiflidir.': 291, '##zin': 142, 'E': 11, '##id': 138, '##ıyoru': 95, '##har': 107, '##ba': 113, '##la buluşmak her ': 247, '##va ç': 194, '##z.': 99, 'd': 22, '##gü': 112, '##incan ': 208, '## iç': 148, '##n ': 80, '##b': 70, '##bul': 114, '##k mu': 200, '##n yap': 197, '##as': 181, '##m': 52, '##!': 71, '##le': 100, '##n key': 198, '##f': 76, '##dür.': 260, '##ul': 93, '##ken ': 166, 'ö': 42, 'ü': 43, '##malıy': 205, '##iç': 89, '##n ha': 196, '[UNK]': 1, 'Yarın buluşmak için plan yapıyoruz.': 297, 'Diz': 126, '##h': 56, '##buluşmak için plan yap': 271, '## N': 145, 'Eski dostlarla buluşmak her ': 275, '##şlanıyorum.': 254, '##itmek için ': 243, 'v': 38, 'Türk mutfağı dünya çapında ünlüdür.': 298, '##za': 141, '##i har': 204, '##ür': 116, '##an ': 182, '##key': 168, '##plan yap': 257, '##zaman key': 246, '##kler': 167, 'Türk mu': 240, '##t': 54, '## f': 146, '##arkta gezin': 264, '##buluşma': 122, '##incan kahv': 267, 'Köpeğ': 236, '##asıls': 255, '##uşma': 111, '##mis': 164, '##ki ': 165, '##hv': 175, '##ün': 115, '##enkler': 248, '##ş': 58, '##buluşmak her ': 231, '.': 7, 'Okula g': 238, 'Okula gitmek için erken kalkmalıyız.': 294, '##in': 88, '##buluşmak için ': 230, '##ıyor': 94, '##lıy': 156, '##k her ': 201, 'Eski ': 235, '##l.': 150, 'Y': 18, '##ap': 92, 'Yarın buluşmak için plan yap': 281, 't': 36, '##i iz': 203, '##ya ç': 187, '##e iç': 162, '##ze': 140, '##i ': 85, '##va çok güze': 262, 'Köpeğimle p': 276, '##is': 136, '##ünlü': 226, 'ş': 46, '##ezin': 161, 'n': 31, 'u': 37, '##ağ': 180, '##os': 177, 'Merhaba! Nasıls': 277, '##en ': 102, 'Bugün ha': 232, '##ika': 139, '##ıls': 186, '##ey': 159, '##er': 82, '##ı d': 185, 'ğ': 44, '##e içer misin?': 284, '##ls': 151, '##er ': 119, 'm': 30, '##har r': 221, '##man key': 206, '##erken kalk': 263, '?': 8, '##imle p': 242, '##ın ': 183, '##lid': 157, ' ': 5, '##dos': 193, '##şla': 179, '##y': 61, '##apında ünlü': 268, '##in ': 98, '##ka': 90, '##nıyorum.': 173, '##le p': 216, '##kten ho': 219, '##la ': 153, '##ok güze': 253, 'Sonbahar renkler': 279, 'Dizi iz': 234, '##tf': 169, '##yapıyorum.': 223, '##tfağ': 250, 'Türk mutfağı dünya çapında ünlü': 296, 'Yar': 133, '##a ': 81, '##iflid': 244, 's': 35, 'Bu': 124, '##ıyoruz.': 213, 'D': 10, 'h': 26, '##! Nasıls': 258, '##apın': 211, 'y': 39, '##in?': 207, '##ın': 109, '##or': 91, '##ı dünya ç': 256, '##e içer mis': 249, '##if': 137, '##z': 48, '##kahv': 210, '##k iç': 120, '##erken ': 199, '##p': 68, '##mu': 163, '##ıyorum.': 121, '##ı dünya çapında ünlü': 285, '##mek için ': 218, '[MASK]': 4, 'N': 14, 'Sonbahar renkleri harika.': 295, '##o': 57, '##a ç': 118, '##ma': 86, 'B': 9, '##ir': 97, '##ç': 66, '##ika.': 245, '##arkt': 202, '##r': 62, '##şma': 108, '##ü': 75, '##me': 103, '##ula g': 212, '##har renkler': 270, 'ı': 45, '##k': 53, '##arkta gezinti yapıyorum.': 287, '##la': 101, '##dostlarla buluşmak her ': 261, '##pla': 188, 'Türk mutfağ': 280, 'Dizi izlemekten hoşlanıyorum.': 290, '##n': 55, '##en': 158, '##da ': 191, '##m.': 104, 'Bugün hava çok güzel.': 288, '##g': 65, '##kt': 105, 'O': 15, 'e': 23, '##dür': 192, '[SEP]': 3, 'Köpeğimle parkta gezinti yapıyorum.': 292, '[PAD]': 0, 'Merhaba! Nasılsın?': 293, '[CLS]': 2, '##i harika.': 265, '##ok ': 178, '##a': 59, '##.': 64, 'ç': 41, 'k': 28, 'z': 40, '##yap': 110, '##ğ': 69, 'g': 25, '##can ': 195, '##ti yapıyorum.': 251, 'i': 27, '##a gezin': 228, '##e': 51, '##im': 134, '##c': 79, '##ünya ç': 227, 'Bir fincan kahv': 273, '##nba': 172, 'Bir fincan kahve içer misin?': 289, '##gün ha': 225, '##tlarla buluşmak her ': 252, '##ıy': 87, '##it': 135, '##ı': 60, '##lemekten ho': 269, '##her ': 176, '## p': 144, '##tlar': 171, 'l': 29}\n",
            "\n",
            "Step 3: Tokenized Pretraining Data:\n",
            "[[30, 82, 220, 71, 49, 55, 255, 222], [20, 63, 225, 262, 150], [39, 84, 183, 271, 213], [36, 116, 200, 250, 285, 260], [28, 67, 189, 242, 287], [20, 97, 146, 267, 284], [32, 53, 212, 243, 286], [35, 57, 172, 270, 265], [22, 96, 203, 269, 254], [23, 73, 165, 261, 283]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. MORPHOLOGICAL TOKENIZER"
      ],
      "metadata": {
        "id": "KQCW_dysw7pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zemberek for TurkishMorphology**"
      ],
      "metadata": {
        "id": "wUCXQf2etKLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zemberek-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GMTQljwLWmS",
        "outputId": "7b69c9e2-bc01-47ee-c3a0-2f14dcb627ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zemberek-python\n",
            "  Downloading zemberek_python-0.2.3-py3-none-any.whl (95.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8 (from zemberek-python)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from zemberek-python) (1.22.4)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=5b385faa33ac93118f40e9f502d50d773836bdaef63309c1cc8dd03d652aaacb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, zemberek-python\n",
            "Successfully installed antlr4-python3-runtime-4.8 zemberek-python-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, decoders\n",
        "import unicodedata\n",
        "import re\n",
        "from zemberek import TurkishMorphology\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ... (Rest of the code from the previous example)\n",
        "\n",
        "# Step 3: Processing the corpus with the trained tokenizer to obtain a tokenized pretraining data\n",
        "def preprocess_with_tokenizer(tokenizer, morphology, text):\n",
        "    # Lowercase conversion\n",
        "    text = text.lower()\n",
        "\n",
        "    # NFC normalization\n",
        "    normalized_text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    # Remove HTML tags using BeautifulSoup\n",
        "    soup = BeautifulSoup(normalized_text, \"html.parser\")\n",
        "    cleaned_text = soup.get_text()\n",
        "\n",
        "    # Remove non-character symbols using regex\n",
        "    cleaned_text = re.sub(r'[0123456789,/\\.!?:‘’()\"]', '', cleaned_text)\n",
        "\n",
        "    # Remove stopwords in Turkish\n",
        "    stop_words = set(stopwords.words(\"turkish\"))\n",
        "    words = cleaned_text.split()\n",
        "    filtered_text = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Tokenize the filtered text\n",
        "    encoded_text = tokenizer.encode(\" \".join(filtered_text))\n",
        "\n",
        "    # Morphological-level tokenization using Zemberek\n",
        "    tokenized_text = []\n",
        "    for token in encoded_text.tokens:\n",
        "        analysis_results = morphology.analyze(token)\n",
        "        for analysis in analysis_results:\n",
        "            morphemes = analysis.get_stem()\n",
        "            tokenized_text.append(morphemes)\n",
        "\n",
        "    return tokenized_text\n",
        "\n",
        "# Sample usage\n",
        "corpus = [\n",
        "    \"Merhaba! Nasılsın?\",\n",
        "    \"Bugün hava çok güzel.\",\n",
        "    \"Yarın buluşmak için plan yapıyoruz.\",\n",
        "    \"Türk mutfağı dünya çapında ünlüdür.\",\n",
        "    \"Köpeğimle parkta gezinti yapıyorum.\",\n",
        "    \"Bir fincan kahve içer misin?\",\n",
        "    \"Okula gitmek için erken kalkmalıyız.\",\n",
        "    \"Sonbahar renkleri harika.\",\n",
        "    \"Dizi izlemekten hoşlanıyorum.\",\n",
        "    \"Eski dostlarla buluşmak her zaman keyiflidir.\"\n",
        "]\n",
        "\n",
        "# Step 1: Applying normalization to clear invalid characters from the text\n",
        "corpus_normalized = [apply_normalization(text) for text in corpus]\n",
        "print(\"Step 1: Applying normalization to clear invalid characters:\")\n",
        "print(corpus_normalized)\n",
        "\n",
        "# Step 2: Training the tokenizer according to the predetermined vocabulary size\n",
        "vocab_size = 1000  # You can adjust the vocabulary size based on your requirements\n",
        "tokenizer = train_tokenizer(corpus_normalized, vocab_size)\n",
        "print(\"\\nStep 2: Trained Tokenizer:\")\n",
        "print(tokenizer.get_vocab())\n",
        "\n",
        "# Step 3: Initialize Zemberek TurkishMorphology\n",
        "morphology = TurkishMorphology.create_with_defaults()\n",
        "\n",
        "# Step 3: Processing the corpus with the trained tokenizer to obtain a tokenized pretraining data\n",
        "preprocessed_data = [preprocess_with_tokenizer(tokenizer, morphology, text) for text in corpus_normalized]\n",
        "print(\"\\nStep 3: Tokenized Pretraining Data:\")\n",
        "print(preprocessed_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2C6D0S4LWo2",
        "outputId": "40779d41-43ea-4c40-aa8c-e4cb959fb380"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Applying normalization to clear invalid characters:\n",
            "['Merhaba! Nasılsın?', 'Bugün hava çok güzel.', 'Yarın buluşmak için plan yapıyoruz.', 'Türk mutfağı dünya çapında ünlüdür.', 'Köpeğimle parkta gezinti yapıyorum.', 'Bir fincan kahve içer misin?', 'Okula gitmek için erken kalkmalıyız.', 'Sonbahar renkleri harika.', 'Dizi izlemekten hoşlanıyorum.', 'Eski dostlarla buluşmak her zaman keyiflidir.']\n",
            "\n",
            "Step 2: Trained Tokenizer:\n",
            "{'ul': 60, 'e ': 67, 'bul': 65, 'ı dünya çapında ': 234, 'a': 19, 'buluşma': 89, 'a g': 82, 'apında ': 176, 'k için plan ': 200, 'dür.': 186, 'e içer ': 187, 'şlanıyorum.': 235, 'ey': 115, 'k her ': 174, 'mu': 145, 'k için ': 88, 'eri ': 172, 'do': 110, 'T': 17, 'iz': 73, '[PAD]': 0, 'a çapında ': 199, 'enkl': 216, 'B': 9, 'i': 27, 'in': 56, 'N': 14, 'gün ha': 190, 'Dizi izlemekten ho': 243, 'şlan': 169, 'ç': 41, 'st': 158, 'ti ': 159, 'ı dü': 167, 'uş': 79, 'Y': 18, 'K': 12, 'ika': 127, 'r.': 156, 'ez': 116, 'tfağ': 160, 'ar': 49, 'har renkl': 239, 'nlü': 151, 'haba! N': 191, 'fağ': 120, 'fl': 119, '[MASK]': 4, 'idir': 128, ' r': 92, 'Merhaba! Nasılsın?': 262, 'iç': 57, 'parkta g': 230, 'incan ': 178, 'd': 22, 'va çok güzel': 231, 'inti ': 179, 'Eski dostlarla buluşmak her zaman key': 260, 'rk ': 157, 'imle ': 218, '[SEP]': 3, 'ö': 42, 'z': 40, 'ı dünya çapında ünlüdür.': 254, 'c': 21, 's': 35, 'ın ': 165, 'ok güzel': 229, 'm': 30, 'parkta gezinti yapıyorum.': 253, 'ası': 108, 'zama': 162, 'Bir fincan kahve içer ': 242, 'har r': 192, 'a ': 48, 'Dizi izlemekten hoşlanıyorum.': 259, 'Okula g': 210, 'buluşmak için plan yapıyoruz.': 240, 'ında ': 168, ' dü': 93, 'ezinti yapıyorum.': 251, 'r': 34, 'kl': 130, 'ıyorum.': 86, 'Bir': 96, 'kalkma': 182, 'b': 20, 'Yarın buluşmak için plan yapıyoruz.': 250, 'Türk ': 212, 'k iç': 85, 'ıy': 54, 'a ç': 83, 'gü': 69, 'ok ': 152, 'Eski dost': 244, 'is': 125, '?': 8, '[CLS]': 2, 'itmek için ': 219, 'içer ': 180, 'e': 23, 'en ': 68, 'el': 113, 'Türk mutfağ': 249, 'ir': 72, 'nba': 149, 'Merhaba! N': 209, 'Eski ': 207, 'kahv': 181, 'erken kalkmalıyız.': 256, '!': 6, 'Sonbahar renkl': 248, 'n ': 47, 'lü': 135, 'n key': 171, '! N': 94, 'her ': 123, 'or': 59, 'lan': 142, 'Diz': 97, 'z.': 81, 'en': 114, ' ': 5, 'lemekten ho': 225, 'Okula gitmek için ': 247, 'ny': 148, 'ma': 53, 'Bugün hava çok güzel.': 257, 't': 36, 'can ': 109, 'l': 29, 'ifl': 129, 'la buluşmak her ': 222, 'So': 102, 'er': 50, 'Merhaba! Nasılsı': 246, 'ken ': 133, ' fincan ': 203, 'güzel': 189, 'n ha': 170, 'hv': 122, 'Dizi iz': 206, 'kta g': 194, 'Sonba': 211, 'Türk mutfağı dünya çapında ünlüdür.': 265, 'ünlüdür.': 233, '[UNK]': 1, 'ezinti ': 217, 'Köpeğimle ': 245, 'peğ': 154, 'ka': 58, 'ı': 45, 'Köpeğimle parkta gezinti yapıyorum.': 261, 'Eski dostlarla buluşmak her zaman keyiflidir.': 266, 'kahve içer ': 238, 'le ': 140, 'nda ': 150, 'leme': 143, 'lıyız.': 224, 'zel': 163, 'lkma': 144, 'Kö': 99, 'Es': 98, 'ha': 70, 'da ': 111, 'v': 38, 'ş': 46, 'idir.': 220, 'n': 31, 'im': 124, 'lsı': 141, 'Tü': 103, 'va ç': 161, 'me': 77, 'ız.': 166, 'larla buluşmak her ': 223, 'par': 153, 'har': 71, 'la ': 136, 'en ho': 188, 'Bugün ha': 204, 'er ': 84, 'kt': 75, 'zaman key': 232, ' f': 91, 'g': 25, 'dü': 66, 'S': 16, 'erken ': 173, 'ba': 64, 'dost': 215, 'larla buluşmak her zaman key': 252, 'Mer': 100, 'erken kalkma': 237, 'kma': 132, 'M': 13, 'sı': 78, 'Bugün hava çok güzel': 241, 'ap': 55, 'k': 28, '.': 7, 'ama': 107, 'Yar': 104, 'O': 15, 'n?': 147, 'plan ': 155, 'asılsı': 214, 'eri harika': 236, 'nya çapında ': 228, 'mis': 146, 'p': 33, 'buluş': 87, 'eğ': 117, 'ağ': 106, 'm.': 76, 'ıyoruz.': 184, 'an': 105, 'y': 39, 'lan ': 139, 'Okula gitmek için erken kalkmalıyız.': 263, 'ğ': 44, 'ba! N': 185, 'mek için ': 196, 'eri harika.': 255, 'Köpeğ': 208, 'yapıyoruz.': 198, 'kten ho': 195, 'Bir fincan kahve içer misin?': 258, 'ki ': 131, 'f': 24, 'in?': 177, 'lıy': 138, 'ıyoru': 62, 'lar': 137, 'harika': 193, 'Bir fincan ': 205, 'i iz': 175, 'Ok': 101, 'Sonbahar renkleri harika.': 264, 'o': 32, 'misin?': 227, ' N': 90, 'dir': 112, 'D': 10, 'k ': 51, 'i ': 52, 'it': 126, 'iflidir.': 221, 'an ': 63, 'u': 37, 'in ': 74, 'ünlü': 164, 'Bu': 95, 'yapıyorum.': 197, 'yap': 80, 'ho': 121, 'buluşmak her ': 201, 'key': 134, 'h': 26, 'eme': 118, 'buluşmak için plan ': 202, 'Yarın ': 213, 'mutfağ': 226, 'E': 11, 'ıyor': 61, 'ula g': 183, 'ü': 43}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:zemberek.morphology.turkish_morphology:TurkishMorphology instance initialized in 19.655850172042847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 13:35:19,072 - zemberek.morphology.turkish_morphology - INFO\n",
            "Msg: TurkishMorphology instance initialized in 19.655850172042847\n",
            "\n",
            "\n",
            "Step 3: Tokenized Pretraining Data:\n",
            "[['m', 'er', 'er', 'er', 'er', 'er', 'ha', 'ha', 'ha', 'b', 'a', 'a', 'n', 'asıl', 'asıl', 'n'], ['b', 'u', 'v', 'a', 'a', 'güzel', 'güzel', 'güzel'], ['y', 'ar', 'ar', 'ın', 'buluş', 'buluş', 'k', 'plan', 'yap', 'z'], ['t', 'rk', 'ün', 'dü', 'r'], ['k', 'ö', 'imle', 'im', 'ezinti', 'yap', 'm'], ['b', 'ir', 'fincan', 'mis', 'in', 'in'], ['o', 'o', 'o', 'o', 'o', 'k', 'it', 'it', 'me', 'k', 'ı', 'z'], ['s', 'o', 'o', 'o', 'o', 'o', 'nba'], ['d', 'iz', 'm'], ['e', 'e', 's', 'ki', 'dost', 'dost', 'la', 'la', 'buluş', 'buluş', 'k']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Evaluation of Translation**"
      ],
      "metadata": {
        "id": "njmnbdI8KCQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. BLUE Score"
      ],
      "metadata": {
        "id": "5RrtOI2jTdBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"references\": [\n",
        "    \"Reference sentence 1\",\n",
        "    \"Reference sentence 2\",\n",
        "    ...\n",
        "  ],\n",
        "  \"model_outputs\": [\n",
        "    \"Model output sentence 1\",\n",
        "    \"Model output sentence 2\",\n",
        "    ...\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "tHY0JU10RApZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def read_json_document(json_path):\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "json_document_path = \"path_to_your_json_document.json\"\n",
        "json_data = read_json_document(json_document_path)"
      ],
      "metadata": {
        "id": "_vYmpVETRBKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "# Tokenize the reference and model output sentences\n",
        "reference_sentences = [tokenize_sentence(sentence) for sentence in json_data[\"references\"]]\n",
        "model_output_sentences = [tokenize_sentence(sentence) for sentence in json_data[\"model_outputs\"]]"
      ],
      "metadata": {
        "id": "cCRs6yOJREGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def calculate_bleu(candidate, references):\n",
        "    # candidate: the machine-translated text as a list of tokens (words)\n",
        "    # references: a list of reference translations, each as a list of tokens (words)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    smoothie = SmoothingFunction().method1  # Smoothing function for short sentences\n",
        "   # smoothie = SmoothingFunction().method4 # Smoothing function for long sentences\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu(references, candidate, smoothing_function=smoothie)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "# Evaluate BLEU score for each pair of reference and model output sentences\n",
        "for reference, model_output in zip(reference_sentences, model_output_sentences):\n",
        "    bleu_score = calculate_bleu(model_output, [reference])\n",
        "    print(\"BLEU score:\", bleu_score)"
      ],
      "metadata": {
        "id": "ULvdJOq8RF5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes for method for smoothing function:\n",
        "\n",
        "method0: Suitable for all sentence lengths, no added n-grams for zero counts.\n",
        "\n",
        "method1: Effective for short sentences (4 tokens or fewer).\n",
        "\n",
        "method2: Effective for sentences with 4 tokens or more.\n",
        "\n",
        "method4: Effective for longer sentences.\n",
        "\n",
        "In practice, it's a good idea to experiment with different smoothing methods and see which one works best for your specific dataset and translation model."
      ],
      "metadata": {
        "id": "N-f_7uOBSDWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "def calculate_bleu(candidate, references):\n",
        "    # candidate: the machine-translated text as a string\n",
        "    # references: a list of reference translations, each as a string\n",
        "\n",
        "    # Tokenize the candidate and references\n",
        "    candidate_tokens = candidate.split()\n",
        "    reference_tokens = [ref.split() for ref in references]\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = bleu.sentence_bleu(reference_tokens, candidate_tokens)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "# Example usage with a JSON data object\n",
        "json_data = {\n",
        "    \"candidates\": [\n",
        "        \"hello how are you\",\n",
        "        \"what is your name\"\n",
        "    ],\n",
        "    \"references\": [\n",
        "        [\"hi how are you\", \"hello how are you\", \"how do you do\"],\n",
        "        [\"what's your name\", \"what is your name\", \"tell me your name\"]\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Calculate BLEU score for each candidate-reference pair\n",
        "bleu_scores = []\n",
        "for candidate, references in zip(json_data[\"candidates\"], json_data[\"references\"]):\n",
        "    # Calculate BLEU score for the candidate-reference pair\n",
        "    bleu_score = calculate_bleu(candidate, references)\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "print(\"BLEU score:\", bleu_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTSo6I0BtnQ5",
        "outputId": "26ecccc3-e2cb-47dc-d77c-f7a2585a628b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: [1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. METEOR Score"
      ],
      "metadata": {
        "id": "d1sM3JUnTY_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "METEOR provides a more comprehensive evaluation than BLEU, taking into account word order, synonyms, and paraphrases."
      ],
      "metadata": {
        "id": "VM1RciWzTmBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfprjunST6Gg",
        "outputId": "d0ed958d-4afa-49f0-f9ca-a446be93c53f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    # Tokenize the sentence\n",
        "    return nltk.word_tokenize(sentence.lower())\n",
        "\n",
        "def calculate_meteor(candidate, references):\n",
        "    # Calculate METEOR score\n",
        "    meteor_score_value = meteor_score(references, candidate)\n",
        "\n",
        "    return meteor_score_value\n",
        "\n",
        "# Example usage\n",
        "def process_json_data(json_data):\n",
        "    candidates = json_data[\"candidates\"]\n",
        "    reference_sentences = json_data[\"references\"]\n",
        "\n",
        "    # Tokenize candidate translations\n",
        "    tokenized_candidates = [preprocess_sentence(candidate) for candidate in candidates]\n",
        "\n",
        "    # Tokenize reference translations\n",
        "    tokenized_references = [list(map(preprocess_sentence, references)) for references in reference_sentences]\n",
        "\n",
        "    # Calculate METEOR score for each candidate\n",
        "    meteor_scores = []\n",
        "    for candidate, references in zip(tokenized_candidates, tokenized_references):\n",
        "        meteor_score_value = calculate_meteor(candidate, references)\n",
        "        meteor_scores.append(meteor_score_value)\n",
        "\n",
        "    return meteor_scores\n",
        "\n",
        "# Example usage with a JSON data object\n",
        "json_data = {\n",
        "    \"candidates\": [\n",
        "        \"hello how are you\",\n",
        "        \"what is your name\"\n",
        "    ],\n",
        "    \"references\": [\n",
        "        [\"hi how are you\", \"hello how are you\", \"how do you do\"],\n",
        "        [\"what's your name\", \"what is your name\", \"tell me your name\"]\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "5wCLCc5BSdxc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor_scores = process_json_data(json_data)\n",
        "print(\"METEOR scores:\", meteor_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyjfZR0Zc-gb",
        "outputId": "b2f4bdc2-b46d-4345-fc7d-9cb9cb4be116"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR scores: [0.9921875, 0.9921875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3. ROUGE Score"
      ],
      "metadata": {
        "id": "lc3qvk6ylWej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxeUIgyORgyz",
        "outputId": "6602165e-fbf8-492e-aac7-b34942ff8123"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.65.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d032b4c1b837b0d1c0db08c550c7529baee5375e434cd46fe400d3e15da9cbd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge(candidate, references, rouge_type='rougeL'):\n",
        "    # candidate: the machine-translated text as a string\n",
        "    # references: a list of reference translations, each as a string\n",
        "    # rouge_type: the type of ROUGE score to compute (default is ROUGE-L)\n",
        "\n",
        "    # Create a ROUGE scorer\n",
        "    rouge = rouge_scorer.RougeScorer([rouge_type])\n",
        "\n",
        "    # Calculate ROUGE score\n",
        "    scores = rouge.score(candidate, references)\n",
        "\n",
        "    return scores[rouge_type].fmeasure\n",
        "\n",
        "# Example usage with a JSON data object\n",
        "json_data = {\n",
        "    \"candidates\": [\n",
        "        \"hello how are you\",\n",
        "        \"what is your name\"\n",
        "    ],\n",
        "    \"references\": [\n",
        "        [\"hi how are you\", \"hello how are you\", \"how do you do\"],\n",
        "        [\"what's your name\", \"what is your name\", \"tell me your name\"]\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Calculate ROUGE score for each candidate-reference pair\n",
        "rouge_scores = []\n",
        "for candidate, references in zip(json_data[\"candidates\"], json_data[\"references\"]):\n",
        "    # Join the list of references into a single string\n",
        "    reference_text = \" \".join(references)\n",
        "\n",
        "    # Calculate ROUGE score for the candidate-reference pair\n",
        "    rouge_score = calculate_rouge(candidate, reference_text)\n",
        "    rouge_scores.append(rouge_score)\n",
        "\n",
        "# Compute the average ROUGE score across all candidate-reference pairs\n",
        "print(\"METEOR scores:\", rouge_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9Td2I6gldd5",
        "outputId": "dbacfd30-bf5f-411c-ae7a-11d9b3eef4d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR scores: [0.5, 0.5]\n"
          ]
        }
      ]
    }
  ]
}